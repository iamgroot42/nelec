{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D, Layer, Dense, Embedding, LSTM, GRU, Dropout, SpatialDropout1D, Input, Average, Bidirectional\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from keras.models import load_model\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't hog GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to training and testing data file. This data can be downloaded from a link, details of which will be provided.\n",
    "trainDataPath = \"./train.txt\"\n",
    "testDataPath = \"./dev.txt\"\n",
    "# Output file that will be generated. This file can be directly submitted.\n",
    "solutionPath = \"./test.txt\"\n",
    "# Path to directory where GloVe file is saved.\n",
    "gloveDir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4                 # Number of classes - Happy, Sad, Angry, Others\n",
    "MAX_NB_WORDS = None                # To set the upper limit on the number of tokens extracted using keras.preprocessing.text.Tokenizer \n",
    "MAX_SEQUENCE_LENGTH = 25         # All sentences having lesser number of words than this will be padded\n",
    "EMBEDDING_DIM = 100               # The dimension of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanText(text):\n",
    "    text = re.sub(r\"\\bu\\b\", \"you\", text)\n",
    "    text = re.sub(r\"\\bm\\b\", \"am\", text)\n",
    "    text = re.sub(r\"\\bn\\b\", \"and\", text)\n",
    "    text = re.sub(r\"\\bluv\\b\", \"love\", text)\n",
    "    text = re.sub(r\"\\bans\\b\", \"answer\", text)\n",
    "    text = re.sub(r\"\\bwt\\b\", \"what\", text)\n",
    "    text = re.sub(r\"\\br\\b\", \"are\", text)\n",
    "    text = re.sub(r\"\\bur\\b\", \"your\", text)\n",
    "    text = re.sub(r\"\\bnthng\\b\", \"nothing\", text)\n",
    "    text = re.sub(r\"\\btxt\\b\", \"text\", text)\n",
    "    text = re.sub(r\"\\by\\b\", \"why\", text)\n",
    "    text = re.sub(r\"\\bconvo\\b\", \"conversation\", text)\n",
    "    text = re.sub(r\"\\bdont\\b\", \"do not\", text)\n",
    "    text = re.sub(r\"\\bpl[s]+\\b\", \"please\", text)\n",
    "    text = re.sub(r\"\\bpl[z]+\\b\", \"please\", text)\n",
    "    text = re.sub(r\"\\bim\\b\", \"i am\", text)\n",
    "    text = re.sub(r\"\\bwlcm\\b\", \"welcome\", text)\n",
    "    text = re.sub(r\"\\bi m\\b\", \"i am\", text)\n",
    "    text = re.sub(r\"\\baren't\\b\", \"are not\", text)\n",
    "    text = re.sub(r\"\\bb[e]?coz\\b\", \"because\", text)\n",
    "    text = re.sub(r\"\\bdnt\\b\", \"did not\", text)\n",
    "    text = re.sub(r\"\\bknw\\b\", \"know\", text)\n",
    "    text = re.sub(r\"\\bsry\\b\", \"sorry\", text)\n",
    "    text = re.sub(r\"\\bchating\\b\", \"chatting\", text)\n",
    "    text = re.sub(r\"\\bfrnds\\b\", \"friends\", text)\n",
    "    text = re.sub(r\"\\bsrry\\b\", \"sorry\", text)\n",
    "    text = re.sub(r\"\\burself\\b\", \"yourself\", text)\n",
    "    text = re.sub(r\"&amp;\", \"and\", text)\n",
    "    text = re.sub(r\"&apos;\", \"'\", text)\n",
    "    text = re.sub(r\"\\btal\\b\", \"talk\", text)\n",
    "    text = re.sub(r\"\\bsec\\b\", \"second\", text)\n",
    "    text = re.sub(r\"\\bmin\\b\", \"minute\", text)\n",
    "    text = re.sub(r\"\\bfr\\b\", \"for\", text)\n",
    "    text = re.sub(r\"\\bwrk\\b\", \"work\", text)\n",
    "    text = re.sub(r\"\\bfrm\\b\", \"from\", text)\n",
    "    text = re.sub(r\"\\bwr are\\b\", \"where are\", text)\n",
    "    text = re.sub(r\"\\bwrkng\\b\", \"working\", text)\n",
    "    text = re.sub(r\"\\bmyslf\\b\", \"myself\", text)\n",
    "    text = re.sub(r\"\\bbtr\\b\", \"better\", text)\n",
    "    text = re.sub(r\"\\bdil\\b\", \"heart\", text)\n",
    "    text = re.sub(r\"\\by[a]+r\\b\", \"yaar\", text)\n",
    "    text = re.sub(r\"\\bdon't\\b\", \"do not\", text)\n",
    "    text = re.sub(r\"\\bdonâ€™t\\b\", \"do not\", text)\n",
    "    text = re.sub(r\"\\bi'm\\b\", \"i am\", text)\n",
    "    text = re.sub(r\"\\bit's\\b\", \"it is\", text)\n",
    "    text = re.sub(r\"\\byou're\\b\", \"you are\", text)\n",
    "    text = re.sub(r\"\\byouâ€™re\\b\", \"you are\", text)\n",
    "    text = re.sub(r\"\\bthat's\\b\", \"that is\", text)\n",
    "    text = re.sub(r\"\\bcan't\\b\", \"cannot\", text)\n",
    "    text = re.sub(r\"\\bcanâ€™t\\b\", \"cannot\", text)\n",
    "    text = re.sub(r\"\\bwhat's\\b\", \"what is\", text)\n",
    "    text = re.sub(r\"\\bwhatâ€™s\\b\", \"what is\", text)\n",
    "    text = re.sub(r\"\\bdidn't\\b\", \"did not\", text)\n",
    "    text = re.sub(r\"\\bi'll\\b\", \"i will\", text)\n",
    "    text = re.sub(r\"\\blet's\\b\", \"let us\", text)\n",
    "    text = re.sub(r\"\\bi've\\b\", \"i have\", text)\n",
    "    text = re.sub(r\"\\bwon't\\b\", \"will not\", text)\n",
    "    text = re.sub(r\"\\bdoesn't\\b\", \"does not\", text)\n",
    "    text = re.sub(r\"\\bit'll\\b\", \"it will\", text)\n",
    "    text = re.sub(r\"\\bofcourse\\b\", \"of course\", text)\n",
    "    text = re.sub(r\"\\bbcz\\b\", \"because\", text)\n",
    "\n",
    "    text = re.sub(r\"\\bwe'll\\b\", \"we will\", text)\n",
    "    text = re.sub(r\"\\bwhen's\\b\", \"when is\", text)\n",
    "    text = re.sub(r\"\\bwe've\\b\", \"we have\", text)\n",
    "    text = re.sub(r\"\\bhe's\\b\", \"he is\", text)\n",
    "    text = re.sub(r\"\\bshe's\\b\", \"she is\", text)\n",
    "    text = re.sub(r\"\\bfrnd\\b\", \"friend\", text)\n",
    "    text = re.sub(r\"\\biâ€™m\\b\", \"i am\", text)\n",
    "    text = re.sub(r\"\\bth[a]?nx\\b\", \"thanks\", text)\n",
    "    text = re.sub(r\"\\bthnks\\b\", \"thanks\", text)\n",
    "    text = re.sub(r\"\\bye[s]+\\b\", \"yes\", text)\n",
    "    text = re.sub(r\"\\biâ€™am\\b\", \"i am\", text)\n",
    "    text = re.sub(r\"\\bisn't\\b\", \"is not\", text)\n",
    "    text = re.sub(r\"\\bhaven't\\b\", \"have not\", text)\n",
    "    text = re.sub(r\"\\bhow's\\b\", \"how is\", text)\n",
    "    text = re.sub(r\"\\bhow're\\b\", \"how are\", text)\n",
    "    text = re.sub(r\"\\bhowz\\b\", \"how is\", text)\n",
    "    text = re.sub(r\"\\bwasn't\\b\", \"was not\", text)\n",
    "    text = re.sub(r\"\\bthere's\\b\", \"there is\", text)\n",
    "    text = re.sub(r\"\\bwe're\\b\", \"we are\", text)\n",
    "    text = re.sub(r\"\\byou'll\\b\", \"you will\", text)\n",
    "    text = re.sub(r\"\\bo[k]+\\b\", \"okay\", text)\n",
    "    text = re.sub(r\"\\bcouldn't\\b\", \"could not\", text)\n",
    "    text = re.sub(r\"\\bthey're\\b\", \"they are\", text)\n",
    "    text = re.sub(r\"\\bitâ€™s\\b\", \"it is\", text)\n",
    "    text = re.sub(r\"\\bthatâ€™s\\b\", \"that is\", text)\n",
    "    text = re.sub(r\"\\bain't\\b\", \"is not\", text)\n",
    "    text = re.sub(r\"\\bwho's\\b\", \"who is\", text)\n",
    "    text = re.sub(r\"\\byou've\\b\", \"you have\", text)\n",
    "    text = re.sub(r\"\\bwhere's\\b\", \"where is\", text)\n",
    "    text = re.sub(r\"\\bshouldn't\\b\", \"should not\", text)\n",
    "    text = re.sub(r\"\\bwouldn't\\b\", \"would not\", text)\n",
    "    text = re.sub(r\"\\b'you\\b\", \"you\", text)\n",
    "    text = re.sub(r\"\\bit'd\\b\", \"it would\", text)\n",
    "    text = re.sub(r\"\\bidk\\b\", \"i do not know\", text)\n",
    "    text = re.sub(r\"\\bha[ha]+\\b\", \"haha\", text)\n",
    "    text = re.sub(r\"\\bhe[he]+\\b\", \"haha\", text)\n",
    "    text = re.sub(r\"\\brofl\\b\", \"haha\", text)\n",
    "    text = re.sub(r\"\\blmao\\b\", \"haha\", text)\n",
    "    text = re.sub(r\"\\bweren't\\b\", \"were not\", text)\n",
    "    text = re.sub(r\"\\bone's\\b\", \"one is\", text)\n",
    "    text = re.sub(r\"\\bwhay\\b\", \"what\", text)\n",
    "    text = re.sub(r\"\\bsomthing\\b\", \"something\", text)\n",
    "    \n",
    "    text = re.sub(r\"ğŸ˜‚+\", \"ğŸ˜‚\", text)\n",
    "    text = re.sub(r\"ğŸ˜+\", \"ğŸ˜\", text)\n",
    "    text = re.sub(r\"ğŸ˜+\", \"ğŸ˜\", text)\n",
    "    text = re.sub(r\"ğŸ˜­+\", \"ğŸ˜­\", text)\n",
    "    text = re.sub(r\"ğŸ˜¤+\", \"ğŸ˜¤\", text)\n",
    "    text = re.sub(r\"ğŸ‘+\", \"ğŸ‘\", text)\n",
    "    text = re.sub(r\"ğŸ™‚+\", \"ğŸ™‚\", text)\n",
    "    text = re.sub(r\"ğŸ˜¨+\", \"ğŸ˜¨\", text)\n",
    "    text = re.sub(r\"ğŸ˜ +\", \"ğŸ˜ \", text)\n",
    "    text = re.sub(r\"ğŸ¤”+\", \"ğŸ¤”\", text)\n",
    "    text = re.sub(r\"ğŸ˜…+\", \"ğŸ˜…,\", text)\n",
    "    text = re.sub(r\"ğŸ˜+\", \"ğŸ˜\", text)\n",
    "    text = re.sub(r\"ğŸ˜›+\", \"ğŸ˜›\", text)\n",
    "    text = re.sub(r\"ğŸ˜†+\", \"ğŸ˜†\", text)\n",
    "    text = re.sub(r\"ğŸ˜¢+\", \"ğŸ˜¢\", text)\n",
    "    text = re.sub(r\"ğŸ˜œ+\", \"ğŸ˜œ\", text)\n",
    "    text = re.sub(r\"ğŸ˜ƒ+\", \"ğŸ˜ƒ\", text)\n",
    "    text = re.sub(r\"ğŸ˜Œ+\", \"ğŸ˜Œ\", text)\n",
    "    text = re.sub(r\"ğŸ˜’+\", \"ğŸ˜’\", text)\n",
    "    text = re.sub(r\"ğŸ˜©+\", \"ğŸ˜©\", text)\n",
    "    text = re.sub(r\"ğŸ˜€+\", \"ğŸ˜€\", text)\n",
    "    text = re.sub(r\"ğŸ˜˜+\", \"ğŸ˜˜\", text)\n",
    "    text = re.sub(r\"ğŸ˜«+\", \"ğŸ˜«\", text)\n",
    "    text = re.sub(r\"ğŸ˜„+\", \"ğŸ˜„\", text)\n",
    "    text = re.sub(r\"ğŸ˜¡+\", \"ğŸ˜¡\", text)\n",
    "    text = re.sub(r\"ğŸ˜+\", \"ğŸ˜\", text)\n",
    "    text = re.sub(r\"â¤ï¸+\", \"â¤ï¸\", text)\n",
    "    text = re.sub(r\"ğŸ˜¸+\", \"ğŸ˜¸\", text)\n",
    "    text = re.sub(r\"ğŸ˜Š+\", \"ğŸ˜Š\", text)\n",
    "    \n",
    "    text = re.sub(r\"\\bhe[y]+\\b\", \"hey\", text)\n",
    "    text = re.sub(r\"\\bshutup\\b\", \"shut up\", text)\n",
    "    text = re.sub(r\"\\bwhatsup\\b\", \"what is up\", text)\n",
    "    text = re.sub(r\"\\bintrested\\b\", \"interested\", text)\n",
    "    text = re.sub(r\"\\btbh\\b\", \"to be honest\", text)\n",
    "    text = re.sub(r\"\\btmrw\\b\", \"tomorrow\", text)\n",
    "    text = re.sub(r\"\\byu[p]+\\b\", \"yup\", text)\n",
    "    text = re.sub(r\"\\bdumbass\\b\", \"dumb ass\", text)\n",
    "    text = re.sub(r\"\\byo[u]+\\b\", \"you\", text)\n",
    "    text = re.sub(r\"\\bttyl\\b\", \"talk to you later\", text)\n",
    "    text = re.sub(r\"\\bthts\\b\", \"that is\", text)\n",
    "    text = re.sub(r\"\\bikr\\b\", \"i know right ?\", text)\n",
    "    text = re.sub(r\"\\bthanku\\b\", \"thank you\", text)\n",
    "    text = re.sub(r\"\\b'you\\b\", \"you\", text)\n",
    "    text = re.sub(r\"\\bwhts\\b\", \"what is\", text)\n",
    "    text = re.sub(r\"\\bye[a]+[h]*\\b\", \"yeah\", text)\n",
    "    text = re.sub(r\"\\byou'are\\b\", \"you are\", text)\n",
    "    text = re.sub(r\"\\bypu\\b\", \"you\", text)\n",
    "    text = re.sub(r\"\\byo[u]+\\b\", \"you\", text)\n",
    "    text = re.sub(r\"\\bxâ€‘d\\b\", \"ğŸ˜†\", text)\n",
    "    text = re.sub(r\"\\bryt\\b\", \"right\", text)\n",
    "    text = re.sub(r\"\\banytym\\b\", \"anytime\", text)\n",
    "    text = re.sub(r\"\\bbitch(.*)\\b\", \"bitch\", text)\n",
    "    text = re.sub(r\"\\blyk\\b\", \"like\", text)\n",
    "    text = re.sub(r\"\\bna[h]+\\b\", \"nah\", text)\n",
    "    text = re.sub(r\"\\bnopes\\b\", \"nope\", text)\n",
    "    text = re.sub(r\"\\bto[o]+\\b\", \"too\", text)\n",
    "    text = re.sub(r\"\\b'i\\b\", \"i\", text)\n",
    "    text = re.sub(r\"\\b'you\\b\", \"you\", text)\n",
    "    text = re.sub(r\"\\bnt[n]?g\\b\", \"nothing\", text)\n",
    "    text = re.sub(r\"\\bi\\b\", \"I\", text)\n",
    "    text = re.sub(r\"\\bohkay\\b\", \"oh okay\", text)\n",
    "    text = re.sub(r\"\\b[m]+[e]+\\b\", \"me\", text)\n",
    "    text = re.sub(r\"\\b[o]+[k]+[a]+[y]+\\b\", \"okay\", text)\n",
    "    text = re.sub(r\"\\b[o]+[h]+\\b\", \"oh\", text)\n",
    "    text = re.sub(r\"\\b[b]+[y]+[e]+\\b\", \"bye\", text)\n",
    "    text = re.sub(r\"\\by[a]+\\b\", \"ya\", text)\n",
    "    text = re.sub(r\"\\b[w]+[h]+[y]+\\b\", \"why\", text)\n",
    "    text = re.sub(r\"\\bbday\\b\", \"birthday\", text)\n",
    "    text = re.sub(r\"\\b[w]+[o]+[w]+\\b\", \"wow\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        raw_conversations : All conversations together\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    raw_conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            raw_conv = line[:].strip()\n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            \n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "            \n",
    "            conv = ' <eos> '.join(line[1:4])\n",
    "            \n",
    "            # Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(cleanText(conv.lower()))\n",
    "            raw_conversations.append(raw_conv)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, raw_conversations, labels\n",
    "    else:\n",
    "        return indices, conversations, raw_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeNormalisedData(dataFilePath, texts):\n",
    "    \"\"\"Write normalised data to a file\n",
    "    Input:\n",
    "        dataFilePath : Path to original train/test file that has been processed\n",
    "        texts : List containing the normalised 3 turn conversations, separated by the <eos> tag.\n",
    "    \"\"\"\n",
    "    normalisedDataFilePath = dataFilePath.replace(\".txt\", \"_normalised.txt\")\n",
    "    with io.open(normalisedDataFilePath, 'w', encoding='utf8') as fout:\n",
    "        with io.open(dataFilePath, encoding='utf8') as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                line = line.strip().split('\\t')\n",
    "                normalisedLine = texts[lineNum].strip().split('<eos>')\n",
    "                fout.write(line[0] + '\\t')\n",
    "                # Write the original turn, followed by the normalised version of the same turn\n",
    "                fout.write(line[1] + '\\t' + normalisedLine[0] + '\\t')\n",
    "                fout.write(line[2] + '\\t' + normalisedLine[1] + '\\t')\n",
    "                fout.write(line[3] + '\\t' + normalisedLine[2] + '\\t')\n",
    "                try:\n",
    "                    # If label information available (train time)\n",
    "                    fout.write(line[4] + '\\n')    \n",
    "                except:\n",
    "                    # If label information not available (test time)\n",
    "                    fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    embeddingsIndex = {}\n",
    "    # Load the embedding vectors from ther GloVe file\n",
    "    with io.open(os.path.join(gloveDir, 'glove.6B.%dd.txt' % EMBEDDING_DIM), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "    \n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "    \n",
    "    oov = []\n",
    "    # Minimum word index of any word is 1. \n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    count, total= 0, 0\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        total += 1\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "            count += 1\n",
    "        else:\n",
    "            oov.append(word)\n",
    "    \n",
    "    print(\"Found embedding for\", str((100 * count) / total), \"% embeddings\")\n",
    "    return embeddingMatrix, oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n",
      "Extracting tokens...\n",
      "Found 16269 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "trainIndices, trainTexts, rawtrainTexts, labels = preprocessData(trainDataPath, mode=\"train\")\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "# Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable   \n",
    "# writeNormalisedData(trainDataPath, trainTexts)\n",
    "print(\"Processing test data...\")\n",
    "testIndices, testTexts, rawtestTexts = preprocessData(testDataPath, mode=\"test\")\n",
    "# writeNormalisedData(testDataPath, testTexts)\n",
    "\n",
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(trainTexts)\n",
    "trainSequences = tokenizer.texts_to_sequences(trainTexts)\n",
    "testSequences = tokenizer.texts_to_sequences(testTexts)\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_three(texts, tknzr):\n",
    "    middle, left, right = [], [], []\n",
    "    for text in texts:\n",
    "        l, m, r = text.split(' <eos> ')\n",
    "        middle.append(m)\n",
    "        left.append(l)\n",
    "        right.append(r)\n",
    "    tokenize = lambda x: tknzr.texts_to_sequences(x)\n",
    "    return (tokenize(left), tokenize(middle), tokenize(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l, train_m, train_r = split_into_three(trainTexts, tokenizer)\n",
    "test_l, test_m, test_r = split_into_three(testTexts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating embedding matrix...\n",
      "Found 400000 word vectors.\n",
      "Found embedding for 66.46382691007437 % embeddings\n"
     ]
    }
   ],
   "source": [
    "print(\"Populating embedding matrix...\")\n",
    "embeddingMatrix, oov = getEmbeddingMatrix(wordIndex)\n",
    "oov = [(x, tokenizer.word_counts.get(x, 0)) for x in oov]\n",
    "oov.sort(key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ğŸ˜‚', 1206), ('ğŸ˜­', 302), ('ğŸ˜', 213), ('ğŸ˜¢', 200), ('ğŸ˜', 199), ('ğŸ˜…', 147), ('ğŸ˜', 132), ('ğŸ˜€', 118), ('ğŸ˜ƒ', 111), ('ğŸ˜¡', 111), ('ğŸ˜†', 110), ('ğŸ˜„', 102), ('ğŸ˜Š', 92), (\"i'd\", 82), ('ğŸ˜’', 77), ('ğŸ˜Œ', 71), ('ğŸ˜ ', 70), ('ğŸ˜¤', 68), ('ğŸ™‚', 63), ('ğŸ˜¸', 60), ('ğŸ˜º', 57), ('ğŸ˜«', 55), ('â€‘', 54), ('ğŸ˜œ', 53), ('ğŸ˜¹', 53), ('ğŸ˜©', 53), ('ğŸ‘', 52), ('ğŸ˜˜', 51), ('ohk', 48), ('ğŸ˜‰', 46), ('ğŸ˜½', 43), ('ğŸ˜»', 42), ('ğŸ’”', 35), ('ğŸ˜', 35), ('hurted', 33), ('ğŸ˜‘', 32), ('ğŸ˜', 32), ('ğŸ™', 31), ('emoji', 30), ('â™¥', 29), ('ğŸ˜¿', 28), ('ğŸ˜¾', 28), ('ğŸ˜¬', 27), ('â¤', 26), ('ğŸ˜‹', 25), ('â€‘d', 23), (\"'i\", 23), ('ğŸ˜”', 23), ('ğŸ™„', 23), ('â˜º', 22), ('ğŸ™€', 22), (\"you'd\", 21), ('ğŸ˜¦', 20), ('â˜¹', 20), ('ğŸ˜', 20), ('ğŸ‘', 20), (\"'â€‘\", 20), ('ğŸ˜§', 19), ('â¤ï¸', 18), ('youğŸ˜‚', 17), ('selfie', 17), ('chatbot', 17), (\"'you\", 16), ('ğŸ˜›', 16), ('iÌ‡', 15), ('yrr', 15), ('ğŸ˜¶', 14), ('8â€‘d', 14), ('ğŸ˜', 13), ('ğŸ˜ğŸ˜‚', 13), ('ğŸ¤”', 12), ('ğŸ‘Œ', 12), ('â˜ºï¸', 12), ('meğŸ˜­', 12), ('ãƒ»', 12), ('â™¡', 11), ('ğŸ˜‡', 11), ('yayy', 11), (\"someone's\", 11), ('byy', 10), (\"friend's\", 10), (\"valentine's\", 10), ('â—‹', 9), ('hooo', 9), ('ğŸ˜¢ğŸ˜­', 9), ('ğŸ˜•', 9), ('meğŸ˜‚', 9), ('hahağŸ˜‚', 9), ('ummmm', 8), ('nvm', 8), ('\\U000fe339', 8), ('ğŸ˜¨', 8), ('ğŸ˜‚ğŸ˜', 8), (\"today's\", 8), ('ahaha', 8), (\"life's\", 8), ('funnyğŸ˜‚', 8), ('emojis', 8), ('oho', 8), ('friendzone', 8)]\n"
     ]
    }
   ],
   "source": [
    "print(oov[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solution file...\n",
      "Shape of training data tensor:  (30160, 25)\n",
      "Shape of label tensor:  (30160, 4)\n"
     ]
    }
   ],
   "source": [
    "train_l = pad_sequences(train_l, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_m = pad_sequences(train_m, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_r = pad_sequences(train_r, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Creating solution file...\")\n",
    "test_l = pad_sequences(test_l, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_m = pad_sequences(test_m, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_r = pad_sequences(test_r, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Shape of training data tensor: \", train_l.shape)\n",
    "print(\"Shape of label tensor: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(predictions, ground):\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "    \n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "    \n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "    \n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "    \n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()    \n",
    "    \n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "    \n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "    return accuracy, microPrecision, microRecall, microF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def microF1Loss(ground, predictions):\n",
    "    discretePredictions = K.one_hot(K.argmax(predictions, axis=1), NUM_CLASSES)\n",
    "    \n",
    "    truePositives = K.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = K.sum(K.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = K.sum(K.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall + K.epsilon())\n",
    "        \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall + K.epsilon())\n",
    "    \n",
    "    truePositives = K.sum(truePositives[1:])\n",
    "    falsePositives = K.sum(falsePositives[1:])\n",
    "    falseNegatives = K.sum(falseNegatives[1:])\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall + + K.epsilon())\n",
    "    \n",
    "    return microF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTriModel(embeddingMatrix, learnEmbs=False):\n",
    "    if learnEmbs:\n",
    "        embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH)\n",
    "    else:\n",
    "        embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embeddingMatrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "    \n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embeddingLayer(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))(x)\n",
    "    y = Bidirectional(GRU(32, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))(x)\n",
    "    \n",
    "    atten_1 = Attention(MAX_SEQUENCE_LENGTH)(x) # skip connect\n",
    "    atten_2 = Attention(MAX_SEQUENCE_LENGTH)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n",
    "    conc = Dropout(0.3)(conc)\n",
    "    conc = Dense(32, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.3)(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=conc)\n",
    "    \n",
    "    input_l, input_m, input_r = Input((MAX_SEQUENCE_LENGTH,)), Input((MAX_SEQUENCE_LENGTH,)), Input((MAX_SEQUENCE_LENGTH,))\n",
    "    embed_l, embed_m, embed_r = model(input_l), model(input_m), model(input_r)\n",
    "    output = Average()([embed_l, embed_m, embed_r])\n",
    "    output = Dense(NUM_CLASSES, activation='softmax')(output)\n",
    "    tri_model = Model(inputs=[input_l, input_m, input_r], outputs=output)\n",
    "    tri_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(1e-2),\n",
    "                  metrics=[microF1Loss])\n",
    "    return tri_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Train on 27144 samples, validate on 3016 samples\n",
      "Epoch 1/10\n",
      "27144/27144 [==============================] - 78s 3ms/step - loss: 1.1387 - microF1Loss: nan - val_loss: 0.8243 - val_microF1Loss: 0.5498\n",
      "Epoch 2/10\n",
      "27144/27144 [==============================] - 66s 2ms/step - loss: 0.8749 - microF1Loss: 0.5391 - val_loss: 0.7123 - val_microF1Loss: 0.6584\n",
      "Epoch 3/10\n",
      "27144/27144 [==============================] - 62s 2ms/step - loss: 0.7674 - microF1Loss: 0.6252 - val_loss: 0.6508 - val_microF1Loss: 0.7142\n",
      "Epoch 4/10\n",
      "27144/27144 [==============================] - 59s 2ms/step - loss: 0.7117 - microF1Loss: 0.6636 - val_loss: 0.6149 - val_microF1Loss: 0.7304\n",
      "Epoch 5/10\n",
      "27144/27144 [==============================] - 61s 2ms/step - loss: 0.6721 - microF1Loss: 0.6888 - val_loss: 0.6056 - val_microF1Loss: 0.7377\n",
      "Epoch 6/10\n",
      "27144/27144 [==============================] - 61s 2ms/step - loss: 0.6404 - microF1Loss: 0.7077 - val_loss: 0.5701 - val_microF1Loss: 0.7593\n",
      "Epoch 7/10\n",
      "27144/27144 [==============================] - 59s 2ms/step - loss: 0.6225 - microF1Loss: 0.7199 - val_loss: 0.5917 - val_microF1Loss: 0.7489\n",
      "Epoch 8/10\n",
      "27144/27144 [==============================] - 60s 2ms/step - loss: 0.6279 - microF1Loss: 0.7142 - val_loss: 0.5524 - val_microF1Loss: 0.7698\n",
      "Epoch 9/10\n",
      "27144/27144 [==============================] - 61s 2ms/step - loss: 0.6078 - microF1Loss: 0.7263 - val_loss: 0.5506 - val_microF1Loss: 0.7818\n",
      "Epoch 10/10\n",
      "27144/27144 [==============================] - 59s 2ms/step - loss: 0.5878 - microF1Loss: 0.7371 - val_loss: 0.5487 - val_microF1Loss: 0.7758\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "clr = CyclicLR(base_lr=0.002, max_lr=0.004,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "print(\"Training model\")\n",
    "model = buildTriModel(embeddingMatrix, learnEmbs=False)\n",
    "\n",
    "model.fit([train_l, train_m, train_r], labels, epochs=15, batch_size=256, validation_split=0.1, callbacks= [clr,])\n",
    "model.save(\"ended_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_meta_features(data):\n",
    "    # Compute emoji-based features\n",
    "    emojis = ['ğŸ˜‚', 'ğŸ˜­', 'ğŸ˜', 'ğŸ˜¢', 'ğŸ˜', 'ğŸ˜…', 'ğŸ˜',\n",
    "              'ğŸ˜€', 'ğŸ˜ƒ', 'ğŸ˜¡', 'ğŸ˜„', 'ğŸ˜†', 'ğŸ˜’', 'ğŸ˜Š',\n",
    "              'ğŸ˜Œ', 'ğŸ˜ ', 'ğŸ˜¤', 'ğŸ™‚', 'ğŸ˜º', 'ğŸ˜«', 'ğŸ˜©',\n",
    "              'ğŸ˜¹', 'ğŸ˜œ', 'ğŸ‘', 'ğŸ˜˜', 'ğŸ˜¸', 'ğŸ˜‰', 'ğŸ˜½',\n",
    "              'ğŸ˜»', 'ğŸ˜', 'ğŸ’”', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ™', 'ğŸ˜¾',\n",
    "              'ğŸ˜¿', 'ğŸ˜¬', 'â¤', 'ğŸ˜‹', 'ğŸ™„', 'ğŸ˜”', 'ğŸ™€',\n",
    "              'ğŸ˜', 'ğŸ‘', 'ğŸ˜¦', 'ğŸ˜§', 'â¤ï¸', 'ğŸ˜›', 'ğŸ˜¶',\n",
    "              'ğŸ˜', 'ğŸ‘Œ', 'ğŸ¤”','ğŸ˜‡', 'ğŸ˜¨', 'ğŸ˜¯', 'ğŸ˜³',\n",
    "              'â˜¹ï¸', 'ğŸ’‹', 'ğŸ‘‹', '?', '!', '.']\n",
    "    happy_words = ['happy', 'lol', 'haha', 'enjoy', 'cool', 'glad',\n",
    "                   'smile', 'nice', 'funny', 'wow']\n",
    "    angry_words = ['angry', 'fuck', 'hell', 'shut up', 'bad', 'rude',\n",
    "                  'block', 'stupid']\n",
    "    sad_words   = ['sad', 'sorry', 'miss', 'alone', 'lonely', 'cry'\n",
    "                   'disappointed']\n",
    "    indicator_words = emojis + happy_words + angry_words + sad_words\n",
    "    \n",
    "    word_features = np.zeros((len(data), len(indicator_words)))\n",
    "    for i, text in enumerate(data):\n",
    "        for j, word in enumerate(indicator_words):\n",
    "            word_features[i][j] += text.lower().count(word)\n",
    "    \n",
    "    # Compute CAPS-based features\n",
    "    capital_features = np.zeros((len(data), 1))\n",
    "    for i, text in enumerate(data):\n",
    "        for word in text.split(' '):\n",
    "            if word.isupper():\n",
    "                capital_features[i] += 1\n",
    "    \n",
    "    # Combine metadata-based features\n",
    "    metadata_features = np.concatenate((word_features, capital_features), axis=1)\n",
    "    return metadata_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30160, 87)\n"
     ]
    }
   ],
   "source": [
    "metadata_features = construct_meta_features(rawtrainTexts)\n",
    "print(metadata_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [14823.  4238.  5460.  5503.]\n",
      "False Positives per class :  [ 6. 68. 41. 21.]\n",
      "False Negatives per class :  [125.   5.   3.   3.]\n",
      "Class happy : Precision : 0.984, Recall : 0.999, F1 : 0.991\n",
      "Class sad : Precision : 0.993, Recall : 0.999, F1 : 0.996\n",
      "Class angry : Precision : 0.996, Recall : 0.999, F1 : 0.998\n",
      "Ignoring the Others class, Macro Precision : 0.9910, Macro Recall : 0.9992, Macro F1 : 0.9951\n",
      "Ignoring the Others class, Micro TP : 15201, FP : 130, FN : 11\n",
      "Accuracy : 0.9955, Micro Precision : 0.9915, Micro Recall : 0.9993, Micro F1 : 0.9954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9954907161803713, 0.99152046, 0.9992769, 0.9953835902288041)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier for metadata-based classification\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import f1_score\n",
    "meta_clf = tree.DecisionTreeClassifier()\n",
    "meta_clf.fit(metadata_features, np.argmax(labels, axis=1))\n",
    "getMetrics(meta_clf.predict_proba(metadata_features), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'semeval.pdf'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump tree structure vislualization\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(meta_clf, out_file=None)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"semeval_dtree\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train accuracy with and without emoji-augmentation\n",
    "preds = model.predict([train_l, train_m, train_r], batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_augment = meta_clf.predict_proba(metadata_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [13456.  3071.  4267.  4585.]\n",
      "False Positives per class :  [2614.  831.  732.  604.]\n",
      "False Negatives per class :  [1492. 1172. 1196.  921.]\n",
      "Class happy : Precision : 0.787, Recall : 0.724, F1 : 0.754\n",
      "Class sad : Precision : 0.854, Recall : 0.781, F1 : 0.816\n",
      "Class angry : Precision : 0.884, Recall : 0.833, F1 : 0.857\n",
      "Ignoring the Others class, Macro Precision : 0.8414, Macro Recall : 0.7792, Macro F1 : 0.8091\n",
      "Ignoring the Others class, Micro TP : 11923, FP : 2167, FN : 3289\n",
      "Accuracy : 0.8415, Micro Precision : 0.8462, Micro Recall : 0.7838, Micro F1 : 0.8138\n",
      "\n",
      "True Positives per class :  [14825.  4200.  5454.  5501.]\n",
      "False Positives per class :  [44. 67. 42. 27.]\n",
      "False Negatives per class :  [123.  43.   9.   5.]\n",
      "Class happy : Precision : 0.984, Recall : 0.990, F1 : 0.987\n",
      "Class sad : Precision : 0.992, Recall : 0.998, F1 : 0.995\n",
      "Class angry : Precision : 0.995, Recall : 0.999, F1 : 0.997\n",
      "Ignoring the Others class, Macro Precision : 0.9906, Macro Recall : 0.9958, Macro F1 : 0.9932\n",
      "Ignoring the Others class, Micro TP : 15155, FP : 136, FN : 57\n",
      "Accuracy : 0.9940, Micro Precision : 0.9911, Micro Recall : 0.9963, Micro F1 : 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9940318302387268, 0.99110585, 0.99625295, 0.9936727389718414)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visible performance boost using prior based on emojis\n",
    "getMetrics(preds, labels)\n",
    "print()\n",
    "getMetrics(0.5 * preds + 0.5 * preds_augment, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([test_l, test_m, test_r], batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = construct_meta_features(rawtestTexts)\n",
    "predictions_meta = meta_clf.predict_proba(test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions  = 0.5 * predictions + 0.5 * predictions_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions.argmax(axis=1)\n",
    "\n",
    "with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "    with io.open(testDataPath, encoding=\"utf8\") as fin:\n",
    "        fin.readline()\n",
    "        for lineNum, line in enumerate(fin):\n",
    "            fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "            fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona",
   "language": "python",
   "name": "persona"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
